{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask GP Regression\n",
    "\n",
    "Multitask regression, introduced in [this paper](https://papers.nips.cc/paper/3189-multi-task-gaussian-process-prediction.pdf) learns similarities in the outputs simultaneously. It's useful when you are performing regression on multiple functions that share the same inputs, especially if they have similarities (such as being sinusodial). \n",
    "\n",
    "Given inputs $x$ and $x'$, and tasks $i$ and $j$, the covariance between two datapoints and two tasks is given by\n",
    "\n",
    "$$  k([x, i], [x', j]) = k_\\text{inputs}(x, x') * k_\\text{tasks}(i, j)\n",
    "$$\n",
    "\n",
    "where $k_\\text{inputs}$ is a standard kernel (e.g. RBF) that operates on the inputs.\n",
    "$k_\\text{task}$ is a lookup table containing inter-task covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from Data_Gen_Script import VField\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training and testing data\n",
    "\n",
    "n = 1500 # input size\n",
    "\n",
    "# N=3, D=3\n",
    "x = np.random.rand(n, 3)\n",
    "vfield = VField(N=3, D=3, tgt_loc=np.array([0.2, 0.1, 0.1]),\n",
    "                 tgt_vec=np.array([0.5, 1.0, 1.0]))\n",
    "y = vfield(x)\n",
    "train_x = torch.Tensor(x[:int(0.8*n), :])\n",
    "test_x = torch.Tensor(x[int(0.8*n):, :])\n",
    "# test_x.shape\n",
    "train_y = y[:int(0.8*n), :]\n",
    "test_y = y[int(0.8*n):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=3\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            gpytorch.kernels.RBFKernel(), num_tasks=3, rank=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 1.149\n",
      "Iter 2/100 - Loss: 1.107\n",
      "Iter 3/100 - Loss: 1.067\n",
      "Iter 4/100 - Loss: 1.026\n",
      "Iter 5/100 - Loss: 0.986\n",
      "Iter 6/100 - Loss: 0.946\n",
      "Iter 7/100 - Loss: 0.906\n",
      "Iter 8/100 - Loss: 0.866\n",
      "Iter 9/100 - Loss: 0.825\n",
      "Iter 10/100 - Loss: 0.785\n",
      "Iter 11/100 - Loss: 0.744\n",
      "Iter 12/100 - Loss: 0.702\n",
      "Iter 13/100 - Loss: 0.661\n",
      "Iter 14/100 - Loss: 0.620\n",
      "Iter 15/100 - Loss: 0.578\n",
      "Iter 16/100 - Loss: 0.537\n",
      "Iter 17/100 - Loss: 0.495\n",
      "Iter 18/100 - Loss: 0.454\n",
      "Iter 19/100 - Loss: 0.413\n",
      "Iter 20/100 - Loss: 0.373\n",
      "Iter 21/100 - Loss: 0.333\n",
      "Iter 22/100 - Loss: 0.293\n",
      "Iter 23/100 - Loss: 0.254\n",
      "Iter 24/100 - Loss: 0.216\n",
      "Iter 25/100 - Loss: 0.179\n",
      "Iter 26/100 - Loss: 0.143\n",
      "Iter 27/100 - Loss: 0.109\n",
      "Iter 28/100 - Loss: 0.076\n",
      "Iter 29/100 - Loss: 0.044\n",
      "Iter 30/100 - Loss: 0.015\n",
      "Iter 31/100 - Loss: -0.013\n",
      "Iter 32/100 - Loss: -0.039\n",
      "Iter 33/100 - Loss: -0.062\n",
      "Iter 34/100 - Loss: -0.083\n",
      "Iter 35/100 - Loss: -0.101\n",
      "Iter 36/100 - Loss: -0.117\n",
      "Iter 37/100 - Loss: -0.130\n",
      "Iter 38/100 - Loss: -0.140\n",
      "Iter 39/100 - Loss: -0.148\n",
      "Iter 40/100 - Loss: -0.154\n",
      "Iter 41/100 - Loss: -0.157\n",
      "Iter 42/100 - Loss: -0.158\n",
      "Iter 43/100 - Loss: -0.157\n",
      "Iter 44/100 - Loss: -0.156\n",
      "Iter 45/100 - Loss: -0.153\n",
      "Iter 46/100 - Loss: -0.150\n",
      "Iter 47/100 - Loss: -0.147\n",
      "Iter 48/100 - Loss: -0.144\n",
      "Iter 49/100 - Loss: -0.142\n",
      "Iter 50/100 - Loss: -0.140\n",
      "Iter 51/100 - Loss: -0.140\n",
      "Iter 52/100 - Loss: -0.140\n",
      "Iter 53/100 - Loss: -0.141\n",
      "Iter 54/100 - Loss: -0.142\n",
      "Iter 55/100 - Loss: -0.144\n",
      "Iter 56/100 - Loss: -0.146\n",
      "Iter 57/100 - Loss: -0.149\n",
      "Iter 58/100 - Loss: -0.151\n",
      "Iter 59/100 - Loss: -0.153\n",
      "Iter 60/100 - Loss: -0.155\n",
      "Iter 61/100 - Loss: -0.157\n",
      "Iter 62/100 - Loss: -0.158\n",
      "Iter 63/100 - Loss: -0.159\n",
      "Iter 64/100 - Loss: -0.160\n",
      "Iter 65/100 - Loss: -0.161\n",
      "Iter 66/100 - Loss: -0.161\n",
      "Iter 67/100 - Loss: -0.161\n",
      "Iter 68/100 - Loss: -0.161\n",
      "Iter 69/100 - Loss: -0.161\n",
      "Iter 70/100 - Loss: -0.161\n",
      "Iter 71/100 - Loss: -0.160\n",
      "Iter 72/100 - Loss: -0.160\n",
      "Iter 73/100 - Loss: -0.160\n",
      "Iter 74/100 - Loss: -0.160\n",
      "Iter 75/100 - Loss: -0.160\n",
      "Iter 76/100 - Loss: -0.160\n",
      "Iter 77/100 - Loss: -0.160\n",
      "Iter 78/100 - Loss: -0.160\n",
      "Iter 79/100 - Loss: -0.160\n",
      "Iter 80/100 - Loss: -0.161\n",
      "Iter 81/100 - Loss: -0.161\n",
      "Iter 82/100 - Loss: -0.161\n",
      "Iter 83/100 - Loss: -0.161\n",
      "Iter 84/100 - Loss: -0.162\n",
      "Iter 85/100 - Loss: -0.162\n",
      "Iter 86/100 - Loss: -0.162\n",
      "Iter 87/100 - Loss: -0.162\n",
      "Iter 88/100 - Loss: -0.162\n",
      "Iter 89/100 - Loss: -0.162\n",
      "Iter 90/100 - Loss: -0.162\n",
      "Iter 91/100 - Loss: -0.162\n",
      "Iter 92/100 - Loss: -0.162\n",
      "Iter 93/100 - Loss: -0.162\n",
      "Iter 94/100 - Loss: -0.162\n",
      "Iter 95/100 - Loss: -0.162\n",
      "Iter 96/100 - Loss: -0.162\n",
      "Iter 97/100 - Loss: -0.162\n",
      "Iter 98/100 - Loss: -0.162\n",
      "Iter 99/100 - Loss: -0.162\n",
      "Iter 100/100 - Loss: -0.162\n"
     ]
    }
   ],
   "source": [
    "# train the model hyperparameters\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 2 if smoke_test else 100\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 3]) torch.Size([900, 900])\n",
      "mean:\n",
      " tensor([[4.3849, 5.2057, 4.3876],\n",
      "        [1.8034, 2.4224, 2.2262],\n",
      "        [5.1913, 5.8939, 4.9780],\n",
      "        [5.7490, 6.4342, 5.3938],\n",
      "        [2.5892, 3.1373, 2.8268],\n",
      "        [3.1759, 3.6756, 3.2161],\n",
      "        [2.8989, 3.5296, 3.1113],\n",
      "        [0.8837, 1.3391, 1.3807],\n",
      "        [1.2371, 1.7803, 1.7356],\n",
      "        [3.1492, 3.6672, 3.2647],\n",
      "        [1.9213, 2.5409, 2.2478],\n",
      "        [1.7748, 2.4167, 2.2217],\n",
      "        [1.3245, 1.8439, 1.7654],\n",
      "        [1.2924, 1.8240, 1.7629],\n",
      "        [2.5050, 3.4243, 2.8292],\n",
      "        [1.7997, 2.3359, 2.1699],\n",
      "        [4.8220, 5.4338, 4.6401],\n",
      "        [4.5486, 5.1376, 4.3965],\n",
      "        [3.5196, 4.2405, 3.5285],\n",
      "        [3.6234, 4.1845, 3.6602],\n",
      "        [5.1070, 6.0470, 5.0324],\n",
      "        [1.0917, 1.6054, 1.5993],\n",
      "        [1.1855, 1.6639, 1.6474],\n",
      "        [1.7184, 2.1844, 2.1145],\n",
      "        [1.3175, 1.8654, 1.7766],\n",
      "        [2.8076, 3.2315, 2.8759],\n",
      "        [1.5799, 2.0482, 1.9729],\n",
      "        [2.8744, 3.5374, 2.9029],\n",
      "        [1.7210, 2.3888, 1.9941],\n",
      "        [1.5662, 2.1682, 2.0208],\n",
      "        [1.3359, 1.8413, 1.7957],\n",
      "        [2.5668, 3.1582, 2.7307],\n",
      "        [4.5531, 5.0702, 4.3055],\n",
      "        [2.7116, 3.6277, 3.0566],\n",
      "        [2.4526, 3.2017, 2.8177],\n",
      "        [1.8654, 2.4575, 2.2581],\n",
      "        [1.3192, 1.7885, 1.7676],\n",
      "        [0.7720, 1.3185, 1.3741],\n",
      "        [1.6149, 2.2613, 2.0214],\n",
      "        [2.1855, 2.9711, 2.5790],\n",
      "        [2.4940, 2.9793, 2.6918],\n",
      "        [3.3699, 3.8438, 3.3872],\n",
      "        [1.9148, 2.6614, 2.3000],\n",
      "        [1.4134, 2.1294, 1.8343],\n",
      "        [4.3496, 5.0163, 4.2896],\n",
      "        [3.1997, 3.6956, 3.2978],\n",
      "        [2.6727, 3.5454, 3.0428],\n",
      "        [4.8186, 5.5890, 4.7199],\n",
      "        [1.8306, 2.4233, 2.1658],\n",
      "        [1.8452, 2.4260, 2.1747],\n",
      "        [3.9230, 4.5204, 3.8663],\n",
      "        [0.9292, 1.3902, 1.4395],\n",
      "        [2.6706, 3.2514, 2.9047],\n",
      "        [2.8263, 3.4062, 3.0090],\n",
      "        [1.2094, 1.8571, 1.6706],\n",
      "        [1.5253, 1.9920, 1.9220],\n",
      "        [1.5493, 2.0514, 1.9739],\n",
      "        [5.0934, 5.9144, 4.9650],\n",
      "        [2.1497, 2.8000, 2.4270],\n",
      "        [0.8864, 1.3769, 1.4351],\n",
      "        [0.3899, 0.9079, 0.9875],\n",
      "        [5.3850, 6.0025, 5.0842],\n",
      "        [2.1063, 2.5983, 2.4037],\n",
      "        [2.9880, 3.5281, 3.1166],\n",
      "        [0.7885, 1.3288, 1.3112],\n",
      "        [1.0957, 1.6665, 1.5935],\n",
      "        [3.1124, 3.6409, 3.2045],\n",
      "        [2.6873, 3.2953, 2.8859],\n",
      "        [1.4188, 1.9005, 1.8201],\n",
      "        [1.4185, 1.9051, 1.8506],\n",
      "        [3.3469, 3.9719, 3.4677],\n",
      "        [2.4299, 2.9134, 2.6231],\n",
      "        [1.0387, 1.6028, 1.5380],\n",
      "        [4.0978, 4.8018, 4.0086],\n",
      "        [2.6991, 3.6429, 3.0499],\n",
      "        [2.8287, 3.7439, 3.1629],\n",
      "        [0.9896, 1.6495, 1.3691],\n",
      "        [0.6882, 1.1484, 1.2184],\n",
      "        [0.7113, 1.1702, 1.2414],\n",
      "        [1.9055, 2.3549, 2.2320],\n",
      "        [1.6773, 2.4044, 2.0958],\n",
      "        [2.9634, 3.8992, 3.2790],\n",
      "        [1.0009, 1.6021, 1.4448],\n",
      "        [3.4638, 4.1279, 3.5856],\n",
      "        [1.9385, 2.3811, 2.2576],\n",
      "        [3.4164, 4.2337, 3.6077],\n",
      "        [2.1171, 2.7489, 2.4514],\n",
      "        [1.8420, 2.5796, 2.2626],\n",
      "        [0.9135, 1.4879, 1.3881],\n",
      "        [2.6380, 3.3275, 2.9386],\n",
      "        [3.5774, 4.2098, 3.5040],\n",
      "        [1.8687, 2.3473, 2.2499],\n",
      "        [3.2711, 3.7670, 3.3506],\n",
      "        [1.8236, 2.6390, 2.0836],\n",
      "        [0.6925, 1.1838, 1.2417],\n",
      "        [2.4082, 3.2003, 2.6601],\n",
      "        [4.1015, 4.6593, 4.0427],\n",
      "        [2.2549, 2.8289, 2.5223],\n",
      "        [0.7218, 1.1756, 1.2453],\n",
      "        [3.7644, 4.2951, 3.7290],\n",
      "        [1.3363, 2.0892, 1.6002],\n",
      "        [5.8687, 6.7515, 5.6122],\n",
      "        [2.1543, 2.7045, 2.4754],\n",
      "        [3.1525, 3.7846, 3.3197],\n",
      "        [1.5684, 2.0196, 2.0038],\n",
      "        [1.3848, 1.9917, 1.8541],\n",
      "        [4.2069, 4.9030, 4.1856],\n",
      "        [0.7657, 1.2370, 1.2970],\n",
      "        [1.0403, 1.5311, 1.6017],\n",
      "        [1.9020, 2.4964, 2.2218],\n",
      "        [2.1314, 2.6988, 2.4630],\n",
      "        [3.3277, 3.8772, 3.4057],\n",
      "        [3.3351, 3.7918, 3.3261],\n",
      "        [1.2139, 1.8201, 1.7053],\n",
      "        [2.0383, 2.6422, 2.3788],\n",
      "        [0.8934, 1.3521, 1.3911],\n",
      "        [1.1766, 1.6368, 1.6356],\n",
      "        [3.0548, 3.6806, 3.1115],\n",
      "        [3.2904, 3.8021, 3.3771],\n",
      "        [3.7480, 4.5415, 3.8777],\n",
      "        [0.9892, 1.4720, 1.4863],\n",
      "        [0.8741, 1.4088, 1.3884],\n",
      "        [3.2620, 3.8373, 3.3389],\n",
      "        [1.2024, 1.7853, 1.6436],\n",
      "        [1.1662, 1.8670, 1.5384],\n",
      "        [2.5151, 2.9523, 2.6670],\n",
      "        [1.3072, 1.7960, 1.8015],\n",
      "        [1.4545, 1.9624, 1.8757],\n",
      "        [1.3895, 2.1288, 1.7359],\n",
      "        [1.2614, 1.8418, 1.7205],\n",
      "        [2.6689, 3.1787, 2.8740],\n",
      "        [6.2392, 6.9503, 5.8183],\n",
      "        [0.9714, 1.5060, 1.5634],\n",
      "        [0.6078, 1.1400, 1.1156],\n",
      "        [2.6518, 3.4051, 2.9720],\n",
      "        [2.5007, 3.2130, 2.6969],\n",
      "        [1.9968, 2.6897, 2.3011],\n",
      "        [1.3288, 1.9456, 1.7295],\n",
      "        [2.3038, 2.8528, 2.5956],\n",
      "        [1.7626, 2.1929, 2.1279],\n",
      "        [5.4307, 6.0604, 5.1153],\n",
      "        [0.5295, 1.0328, 1.0633],\n",
      "        [0.8223, 1.2740, 1.3273],\n",
      "        [0.7502, 1.2597, 1.2708],\n",
      "        [1.9729, 2.5810, 2.3531],\n",
      "        [3.3474, 3.8697, 3.3582],\n",
      "        [2.9998, 3.6234, 3.1696],\n",
      "        [2.0586, 2.8170, 2.2001],\n",
      "        [1.5901, 2.2316, 2.0291],\n",
      "        [4.3762, 5.0203, 4.2927],\n",
      "        [2.2390, 3.0204, 2.3929],\n",
      "        [2.9465, 3.6129, 2.9605],\n",
      "        [1.1716, 1.6558, 1.7079],\n",
      "        [1.6645, 2.1872, 2.0720],\n",
      "        [0.6718, 1.2608, 1.1206],\n",
      "        [4.1458, 5.0437, 4.2167],\n",
      "        [1.0990, 1.6469, 1.5933],\n",
      "        [2.4665, 3.1336, 2.7848],\n",
      "        [1.3588, 2.0226, 1.6887],\n",
      "        [2.8556, 3.4027, 3.0018],\n",
      "        [1.4693, 1.9733, 1.8986],\n",
      "        [2.5530, 3.2936, 2.8533],\n",
      "        [0.7908, 1.2810, 1.3119],\n",
      "        [1.4305, 2.0261, 1.8249],\n",
      "        [2.0852, 2.6718, 2.4142],\n",
      "        [1.9015, 2.4993, 2.2018],\n",
      "        [2.7734, 3.5134, 2.9760],\n",
      "        [0.7262, 1.2295, 1.2860],\n",
      "        [1.0132, 1.4697, 1.4854],\n",
      "        [2.0646, 2.8263, 2.4967],\n",
      "        [1.0200, 1.4879, 1.5172],\n",
      "        [0.5031, 0.9987, 1.0434],\n",
      "        [0.9149, 1.4499, 1.5305],\n",
      "        [1.7328, 2.2652, 2.1226],\n",
      "        [3.5882, 4.5180, 3.7951],\n",
      "        [5.8259, 6.6299, 5.5365],\n",
      "        [2.5632, 3.1378, 2.7933],\n",
      "        [1.5231, 1.9819, 1.9712],\n",
      "        [3.3623, 3.9530, 3.3903],\n",
      "        [3.7749, 4.5135, 3.8716],\n",
      "        [1.3995, 2.0910, 1.8323],\n",
      "        [1.3554, 1.9243, 1.8390],\n",
      "        [1.3620, 1.9391, 1.8282],\n",
      "        [0.7425, 1.2902, 1.2440],\n",
      "        [2.3327, 2.8920, 2.5830],\n",
      "        [3.7902, 4.4370, 3.8331],\n",
      "        [3.0866, 3.7617, 3.1948],\n",
      "        [2.1798, 2.6266, 2.4559],\n",
      "        [1.2708, 1.9340, 1.6233],\n",
      "        [3.0427, 3.5244, 3.0909],\n",
      "        [1.6065, 2.1467, 2.0224],\n",
      "        [1.3176, 1.9047, 1.7608],\n",
      "        [2.4924, 3.0096, 2.7315],\n",
      "        [2.1208, 2.9081, 2.2571],\n",
      "        [3.2265, 4.0844, 3.4965],\n",
      "        [1.0181, 1.6670, 1.4411],\n",
      "        [1.0283, 1.5505, 1.5217],\n",
      "        [2.0479, 2.4927, 2.3142],\n",
      "        [2.9761, 3.7570, 3.1833],\n",
      "        [2.7312, 3.3318, 2.8626],\n",
      "        [3.3189, 3.7688, 3.3246],\n",
      "        [0.6350, 1.2119, 1.0883],\n",
      "        [1.1495, 1.7660, 1.5615],\n",
      "        [0.8080, 1.3303, 1.4274],\n",
      "        [2.0641, 2.6373, 2.3922],\n",
      "        [0.7658, 1.3395, 1.2461],\n",
      "        [1.5004, 2.1732, 1.8076],\n",
      "        [3.9452, 4.6181, 3.8510],\n",
      "        [5.1032, 5.8233, 4.9211],\n",
      "        [1.1272, 1.6176, 1.6104],\n",
      "        [0.3902, 0.8823, 0.9471],\n",
      "        [1.4246, 1.9193, 1.8411],\n",
      "        [2.4236, 2.9299, 2.6808],\n",
      "        [2.9446, 3.4969, 3.1151],\n",
      "        [4.3908, 5.0343, 4.2136],\n",
      "        [1.6814, 2.2616, 2.0987],\n",
      "        [2.2819, 2.9990, 2.6511],\n",
      "        [1.8941, 2.4253, 2.2125],\n",
      "        [1.9330, 2.4789, 2.2736],\n",
      "        [3.0147, 3.6863, 3.2244],\n",
      "        [3.2954, 3.9384, 3.4266],\n",
      "        [2.0687, 2.6177, 2.4021],\n",
      "        [1.2476, 1.9682, 1.5500],\n",
      "        [2.5017, 2.9591, 2.6180],\n",
      "        [3.1590, 3.6031, 3.2026],\n",
      "        [1.7966, 2.3571, 2.1722],\n",
      "        [1.9486, 2.4922, 2.2984],\n",
      "        [3.5644, 4.2894, 3.6947],\n",
      "        [2.1810, 2.8993, 2.4965],\n",
      "        [1.5397, 2.2537, 1.9127],\n",
      "        [3.9330, 4.5757, 3.8604],\n",
      "        [0.8126, 1.3402, 1.3341],\n",
      "        [1.2424, 1.8887, 1.6842],\n",
      "        [3.3935, 4.1508, 3.4960],\n",
      "        [2.4800, 3.1358, 2.7571],\n",
      "        [0.9154, 1.3646, 1.4016],\n",
      "        [1.8413, 2.3152, 2.1810],\n",
      "        [1.5285, 2.1101, 1.9860],\n",
      "        [1.5736, 2.3228, 1.7656],\n",
      "        [1.2468, 1.8120, 1.7129],\n",
      "        [1.8409, 2.3821, 2.2096],\n",
      "        [4.2043, 4.9160, 4.1985],\n",
      "        [3.3873, 3.8621, 3.4302],\n",
      "        [1.5534, 2.0173, 1.8850],\n",
      "        [1.0284, 1.5769, 1.5154],\n",
      "        [2.7903, 3.5204, 3.0809],\n",
      "        [2.6424, 3.5140, 2.9114],\n",
      "        [5.4211, 6.1037, 5.1343],\n",
      "        [2.4719, 2.9443, 2.6975],\n",
      "        [3.2869, 3.8416, 3.3877],\n",
      "        [0.8574, 1.3432, 1.3740],\n",
      "        [1.3221, 1.9452, 1.7891],\n",
      "        [1.2057, 1.6719, 1.6764],\n",
      "        [1.1441, 1.6832, 1.6368],\n",
      "        [1.4676, 1.9435, 1.9040],\n",
      "        [2.7979, 3.6202, 3.1076],\n",
      "        [3.3637, 3.8191, 3.3341],\n",
      "        [1.9123, 2.4189, 2.2670],\n",
      "        [1.1237, 1.7316, 1.5753],\n",
      "        [4.0125, 4.9281, 4.1186],\n",
      "        [1.1004, 1.6059, 1.6461],\n",
      "        [3.6293, 4.5769, 3.8221],\n",
      "        [1.7145, 2.4507, 1.9140],\n",
      "        [1.7941, 2.4424, 2.2384],\n",
      "        [5.0356, 5.8925, 4.9366],\n",
      "        [1.9365, 2.4026, 2.2758],\n",
      "        [2.8853, 3.3058, 2.9220],\n",
      "        [3.9881, 4.9119, 4.1224],\n",
      "        [0.5912, 1.0561, 1.1287],\n",
      "        [2.2762, 2.9466, 2.4796],\n",
      "        [1.6450, 2.1542, 2.0333],\n",
      "        [1.3479, 1.9084, 1.8273],\n",
      "        [3.7707, 4.5469, 3.8876],\n",
      "        [1.2103, 1.8865, 1.5396],\n",
      "        [4.6346, 5.4961, 4.6144],\n",
      "        [0.6750, 1.2319, 1.2189],\n",
      "        [4.2100, 4.8235, 4.1044],\n",
      "        [2.8293, 3.4274, 2.9714],\n",
      "        [2.4729, 3.0864, 2.6742],\n",
      "        [2.7307, 3.2649, 2.8928],\n",
      "        [2.3101, 2.8389, 2.5938],\n",
      "        [0.6630, 1.1231, 1.1945],\n",
      "        [3.5813, 4.1083, 3.6083],\n",
      "        [1.9864, 2.4725, 2.3407],\n",
      "        [3.3507, 3.9363, 3.4096],\n",
      "        [0.6638, 1.1841, 1.1921],\n",
      "        [1.3084, 2.0570, 1.5663],\n",
      "        [2.5083, 3.0177, 2.7492],\n",
      "        [2.9055, 3.5718, 3.1091],\n",
      "        [4.3408, 5.0341, 4.2500],\n",
      "        [1.5042, 2.1813, 1.7877],\n",
      "        [2.8470, 3.2906, 2.8821],\n",
      "        [3.1828, 3.8960, 3.3783],\n",
      "        [2.4806, 3.0791, 2.7101],\n",
      "        [1.9733, 2.7752, 2.2029],\n",
      "        [3.1798, 3.7399, 3.3023],\n",
      "        [2.3675, 3.0906, 2.6307],\n",
      "        [4.0973, 4.7582, 4.0884],\n",
      "        [1.5961, 2.0775, 2.0000],\n",
      "        [1.0155, 1.6492, 1.4311]])\n",
      " covariance:\n",
      " tensor([[ 3.7797e-02,  1.5068e-04,  1.0109e-04,  ..., -2.4796e-05,\n",
      "          1.9073e-05,  1.5259e-05],\n",
      "        [ 1.5450e-04,  4.0188e-02,  9.4414e-05,  ...,  2.0981e-05,\n",
      "         -2.7657e-05,  1.2398e-05],\n",
      "        [ 1.0109e-04,  9.1553e-05,  4.0337e-02,  ...,  1.7166e-05,\n",
      "          1.5259e-05, -3.5286e-05],\n",
      "        ...,\n",
      "        [-2.5749e-05,  1.7166e-05,  1.5259e-05,  ...,  3.7728e-02,\n",
      "          1.0872e-04,  7.7248e-05],\n",
      "        [ 1.6212e-05, -3.0518e-05,  1.5259e-05,  ...,  1.1063e-04,\n",
      "          4.0110e-02,  6.9618e-05],\n",
      "        [ 1.6212e-05,  1.8120e-05, -3.7193e-05,  ...,  7.4387e-05,\n",
      "          6.9618e-05,  4.0269e-02]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with the model\n",
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plots\n",
    "# f, (y1_ax, y2_ax) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad(): #, gpytorch.settings.fast_pred_var():\n",
    "    predictions = likelihood(model(test_x))\n",
    "    mean = predictions.mean\n",
    "    covariance = predictions.covariance_matrix\n",
    "    lower, upper = predictions.confidence_region()\n",
    "\n",
    "print(mean.shape, covariance.shape)\n",
    "print(f\"mean:\\n {mean}\\n covariance:\\n {covariance}\")\n",
    "    \n",
    "# # This contains predictions for both tasks, flattened out\n",
    "# # The first half of the predictions is for the first task\n",
    "# # The second half is for the second task\n",
    "\n",
    "# # Plot training data as black stars\n",
    "# y1_ax.plot(train_x.detach().numpy(), train_y[:, 0].detach().numpy(), 'k*')\n",
    "# # Predictive mean as blue line\n",
    "# y1_ax.plot(test_x.numpy(), mean[:, 0].numpy(), 'b')\n",
    "# # Shade in confidence \n",
    "# y1_ax.fill_between(test_x.numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)\n",
    "# y1_ax.set_ylim([-3, 3])\n",
    "# y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "# y1_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# # Plot training data as black stars\n",
    "# y2_ax.plot(train_x.detach().numpy(), train_y[:, 1].detach().numpy(), 'k*')\n",
    "# # Predictive mean as blue line\n",
    "# y2_ax.plot(test_x.numpy(), mean[:, 1].numpy(), 'b')\n",
    "# # Shade in confidence \n",
    "# y2_ax.fill_between(test_x.numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)\n",
    "# y2_ax.set_ylim([-3, 3])\n",
    "# y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "# y2_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0085, 0.0006, 0.0372]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the model correctly predicts the value of the input location associated with the target\n",
    "loc = torch.Tensor([[0.2, 0.1, 0.1]])\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    pred = likelihood(model(loc))\n",
    "    mean = pred.mean\n",
    "    covar = pred.covariance_matrix\n",
    "\n",
    "error = abs(torch.Tensor([[0.5, 1., 1.]]) - mean)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
